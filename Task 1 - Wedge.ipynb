{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_gbq\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.cloud.exceptions import NotFound\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths\n",
    "file_path = r\"C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\"\n",
    "\n",
    "# Read CSVs in chunks and upload to BigQuery\n",
    "credentials_path = r\"C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project-bt-b14310631abc.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "gbq_proj_id = \"wedge-project-bt\"\n",
    "dataset_id = \"wedge_data\"\n",
    "\n",
    "\n",
    "\n",
    "# # Read CSVs in chunks and upload to BigQuery\n",
    "# credentials_path = r\"C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\umt-msba-gg-key.json\"\n",
    "# credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "# gbq_proj_id = \"umt-msba\"\n",
    "# dataset_id = \"wedge_transactions\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted transArchive_201001_201003.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201004_201006.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201007_201009.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201010_201012.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201101_201103.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201104.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201105.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Task 1 - Wedge.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/Task%201%20-%20Wedge.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m             \u001b[39m# Open the zip file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/Task%201%20-%20Wedge.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m             \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39mZipFile(file_path, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m zip_ref:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/Task%201%20-%20Wedge.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                 \u001b[39m# Extract all the contents into the directory\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/Task%201%20-%20Wedge.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                 zip_ref\u001b[39m.\u001b[39;49mextractall(extract_to_dir)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/Task%201%20-%20Wedge.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                 \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExtracted \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mextract_to_dir\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/Task%201%20-%20Wedge.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAll files have been extracted.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1520.0_x64__qbz5n2kfra8p0\\Lib\\zipfile.py:1681\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[1;34m(self, path, members, pwd)\u001b[0m\n\u001b[0;32m   1678\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(path)\n\u001b[0;32m   1680\u001b[0m \u001b[39mfor\u001b[39;00m zipinfo \u001b[39min\u001b[39;00m members:\n\u001b[1;32m-> 1681\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(zipinfo, path, pwd)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1520.0_x64__qbz5n2kfra8p0\\Lib\\zipfile.py:1736\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[1;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[0;32m   1732\u001b[0m     \u001b[39mreturn\u001b[39;00m targetpath\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(member, pwd\u001b[39m=\u001b[39mpwd) \u001b[39mas\u001b[39;00m source, \\\n\u001b[0;32m   1735\u001b[0m      \u001b[39mopen\u001b[39m(targetpath, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m target:\n\u001b[1;32m-> 1736\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopyfileobj(source, target)\n\u001b[0;32m   1738\u001b[0m \u001b[39mreturn\u001b[39;00m targetpath\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1520.0_x64__qbz5n2kfra8p0\\Lib\\shutil.py:197\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[1;34m(fsrc, fdst, length)\u001b[0m\n\u001b[0;32m    195\u001b[0m fdst_write \u001b[39m=\u001b[39m fdst\u001b[39m.\u001b[39mwrite\n\u001b[0;32m    196\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     buf \u001b[39m=\u001b[39m fsrc_read(length)\n\u001b[0;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[0;32m    199\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1520.0_x64__qbz5n2kfra8p0\\Lib\\zipfile.py:955\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    954\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[1;32m--> 955\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read1(n)\n\u001b[0;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[0;32m    957\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1520.0_x64__qbz5n2kfra8p0\\Lib\\zipfile.py:1031\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_type \u001b[39m==\u001b[39m ZIP_DEFLATED:\n\u001b[0;32m   1030\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMIN_READ_SIZE)\n\u001b[1;32m-> 1031\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(data, n)\n\u001b[0;32m   1032\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39meof \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m                  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_left \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m                  \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n\u001b[0;32m   1035\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the directory where your zip files are located\n",
    "zip_dir = \"C:\\\\Users\\\\britt\\\\OneDrive - The University of Montana\\\\Applied Data Analytics\\\\Wedge Project\\\\WedgeZipOfZips\"\n",
    "\n",
    "# Define the directory where you want to save the unzipped files\n",
    "extract_to_dir = \"C:\\\\Users\\\\britt\\\\OneDrive - The University of Montana\\\\Applied Data Analytics\\\\Wedge Project\\\\wedge-project\\\\Uploaded\"\n",
    "\n",
    "# Walk through the directory\n",
    "for root, dirs, files in os.walk(zip_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.zip'):\n",
    "            # Construct the file path\n",
    "            file_path = os.path.join(root, file)\n",
    "            # Open the zip file\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                # Extract all the contents into the directory\n",
    "                zip_ref.extractall(extract_to_dir)\n",
    "                print(f\"Extracted {file} to {extract_to_dir}\")\n",
    "\n",
    "print(\"All files have been extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for the files\n",
    "headers = [\n",
    "    'datetime', 'register_no', 'emp_no', 'trans_no', 'upc', 'description', 'trans_type', 'trans_subtype', 'trans_status',\n",
    "    'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice', 'altPrice', 'tax', 'taxexempt', 'foodstamp',\n",
    "    'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype', 'voided', 'percentDiscount', 'ItemQtty', 'volDiscType',\n",
    "    'volume', 'VolSpecial', 'mixMatch', 'matched', 'memType', 'staff', 'numflag', 'itemstatus', 'tenderstatus', 'charflag',\n",
    "    'varflag', 'batchHeaderID', 'local', 'organic', 'display', 'receipt', 'card_no', 'store', 'branch', 'match_id', 'trans_id'\n",
    "]\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for file in files: \n",
    "        full_path = os.path.join(root, file) \n",
    "        if file.endswith('.csv'): \n",
    "            with open(full_path, 'r') as f: \n",
    "                first_line = f.readline().strip()\n",
    "\n",
    "            # Check if the file likely has headers based on the first line\n",
    "            if first_line.startswith('\"datetime\"') or first_line.startswith('datetime'):\n",
    "                print(f\"File {file} seems to already have headers. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # If not, then prepend headers to the file\n",
    "            print(f\"Adding headers to {file}\")\n",
    "            with open(full_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            with open(full_path, 'w') as f:\n",
    "                f.write(','.join(headers) + '\\n' + content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded:\n",
      "['transArchive_201001_201003.csv', 'transArchive_201004_201006.csv', 'transArchive_201007_201009.csv', 'transArchive_201010_201012.csv', 'transArchive_201101_201103.csv', 'transArchive_201104.csv', 'transArchive_201105.csv', 'transArchive_201106.csv', 'transArchive_201501_201503.csv']\n",
      "Found CSV file: transArchive_201001_201003.csv\n",
      "Detected delimiter: ,\n",
      "Table 'wedge-project-bt.wedge_data.transArchive_201001_201003' not found, skipping deletion.\n",
      "Reading CSV file in chunks: transArchive_201001_201003.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 1 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 2 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 3 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 4 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 5 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 6 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 7 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 8 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 9 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 10 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 11 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 12 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 13 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 14 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 15 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 16 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 17 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 18 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 19 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 20 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 21 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 22 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 23 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 24 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 25 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 26 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 27 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 28 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\britt\\AppData\\Local\\Temp\\ipykernel_22832\\1431577111.py:71: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: None if x == '' else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunk 29 to transArchive_201001_201003...\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 50000  # Define the chunk size for reading the CSV\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to drop a table if it exists\n",
    "def drop_table_if_exists(dataset_id, table_name, credentials, gbq_proj_id):\n",
    "    client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    table_id = f\"{gbq_proj_id}.{dataset_id}.{table_name}\"\n",
    "    try:\n",
    "        client.delete_table(table_id)  # API request\n",
    "        print(f\"Deleted table '{table_id}'\")\n",
    "    except NotFound:\n",
    "        print(f\"Table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "# Define the detect_delimiter function\n",
    "def detect_delimiter(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        first_line = file.readline()\n",
    "        return \";\" if \";\" in first_line else \",\"\n",
    "\n",
    "# Define the clean_dataframe function\n",
    "def clean_dataframe(df):\n",
    "    # Type Conversion lists\n",
    "    float_columns = [\n",
    "        'register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale', 'cost', \n",
    "        'unitPrice', 'total', 'regPrice', 'altPrice', 'tax', 'taxexempt', 'foodstamp', \n",
    "        'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype', 'voided', \n",
    "        'percentDiscount', 'ItemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', \n",
    "        'matched', 'numflag', 'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', \n",
    "        'receipt', 'card_no', 'store', 'branch', 'match_id', 'trans_id'\n",
    "    ]\n",
    "    boolean_columns = ['memType', 'staff', 'batchHeaderID', 'display']\n",
    "    string_columns = ['upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'charflag']\n",
    "\n",
    "    # Convert string columns first\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    # Trim spaces from string columns and replace double quotes\n",
    "    df[col] = df[col].str.replace('\"', '', regex=False)\n",
    "\n",
    "    # Clean and convert float columns\n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            # Replace non-numeric characters with NaN and convert to float\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Handle missing values in float columns, for example, by filling with 0\n",
    "    df[float_columns] = df[float_columns].fillna(0)\n",
    "\n",
    "    # Convert boolean columns\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "   # Convert datetime\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "\n",
    "    # Replace specific strings with an empty string\n",
    "    replace_strings = [\"\\\\N\", \"\\\\\\\\N\", \"nan\", \"NULL\"]\n",
    "    df.replace(replace_strings, \"\", inplace=True)\n",
    "\n",
    "    # Trim spaces from string columns and replace double quotes\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "            df[col] = df[col].str.replace('\\\\\\\"', '', regex=False)\n",
    "\n",
    "    # Replace empty strings with None\n",
    "    df = df.applymap(lambda x: None if x == '' else x)\n",
    "\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "print(f\"Contents of {file_path}:\")\n",
    "print(os.listdir(file_path))\n",
    "\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "\n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Found CSV file: {file}\")\n",
    "\n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"Detected delimiter: {delimiter}\")\n",
    "\n",
    "            # Reading CSV file with correct handling of quoted fields\n",
    "            chunk_iter = pd.read_csv(full_path, delimiter=delimiter, quotechar='\"', chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "\n",
    "            table_name = file.replace('.csv', '')\n",
    "\n",
    "            # Drop the table if it exists\n",
    "            drop_table_if_exists(dataset_id, table_name, credentials, gbq_proj_id)\n",
    "\n",
    "            print(f\"Reading CSV file in chunks: {file}...\")\n",
    "            for idx, chunk_df in enumerate(chunk_iter):\n",
    "                # Clean the dataframe\n",
    "                chunk_df = clean_dataframe(chunk_df)\n",
    "\n",
    "                # Modify the field names to comply with BigQuery rules\n",
    "                chunk_df.columns = [col.lower().replace(';', '') for col in chunk_df.columns]\n",
    "\n",
    "                print(f\"Uploading chunk {idx + 1} to {table_name}...\")\n",
    "                if idx == 0:\n",
    "                    # For the first chunk, create the table with the defined schema\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='replace', credentials=credentials, table_schema=schema)\n",
    "                else:\n",
    "                    # For subsequent chunks, append to the table\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='append', credentials=credentials)\n",
    "                del chunk_df  # Clear the chunk from memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50000  # Define the chunk size for reading the CSV\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to drop a table if it exists\n",
    "def drop_table_if_exists(dataset_id, table_name, credentials, gbq_proj_id):\n",
    "    client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    table_id = f\"{gbq_proj_id}.{dataset_id}.{table_name}\"\n",
    "    try:\n",
    "        client.delete_table(table_id)  # API request\n",
    "        print(f\"Deleted table '{table_id}'\")\n",
    "    except NotFound:\n",
    "        print(f\"Table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "# Define the detect_delimiter function\n",
    "def detect_delimiter(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        first_line = file.readline()\n",
    "        return \";\" if \";\" in first_line else \",\"\n",
    "\n",
    "# Define the clean_dataframe function\n",
    "def clean_dataframe(df):\n",
    "    # Type Conversion lists\n",
    "    float_columns = [\n",
    "        'register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale', 'cost', \n",
    "        'unitPrice', 'total', 'regPrice', 'altPrice', 'tax', 'taxexempt', 'foodstamp', \n",
    "        'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype', 'voided', \n",
    "        'percentDiscount', 'ItemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', \n",
    "        'matched', 'numflag', 'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', \n",
    "        'receipt', 'card_no', 'store', 'branch', 'match_id', 'trans_id'\n",
    "    ]\n",
    "    boolean_columns = ['memType', 'staff', 'batchHeaderID', 'display']\n",
    "    string_columns = ['upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'charflag']\n",
    "\n",
    "    # Convert string columns first\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    # Trim spaces from string columns and replace double quotes\n",
    "    df[col] = df[col].str.replace('\"', '', regex=False)\n",
    "\n",
    "    # Clean and convert float columns\n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            # Replace non-numeric characters with NaN and convert to float\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Handle missing values in float columns, for example, by filling with 0\n",
    "    df[float_columns] = df[float_columns].fillna(0)\n",
    "\n",
    "    # Convert boolean columns\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "   # Convert datetime\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "    # Replace specific strings with an empty string\n",
    "    replace_strings = [\"\\\\N\", \"\\\\\\\\N\", \"nan\", \"NULL\"]\n",
    "    df.replace(replace_strings, \"\", inplace=True)\n",
    "\n",
    "    # Trim spaces from string columns and replace double quotes\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "            df[col] = df[col].str.replace('\\\\\\\"', '', regex=False)\n",
    "\n",
    "    # Replace empty strings with None\n",
    "    df = df.applymap(lambda x: None if x == '' else x)\n",
    "\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "print(f\"Contents of {file_path}:\")\n",
    "print(os.listdir(file_path))\n",
    "\n",
    "# Define BigQuery schema\n",
    "schema = [\n",
    "    {\"name\": \"datetime\", \"type\": \"TIMESTAMP\"},\n",
    "    {\"name\": \"register_no\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"emp_no\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"trans_no\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"upc\", \"type\": \"STRING\"},\n",
    "    {\"name\": \"description\", \"type\": \"STRING\"},\n",
    "    {\"name\": \"trans_type\", \"type\": \"STRING\"},\n",
    "    {\"name\": \"trans_subtype\", \"type\": \"STRING\"},\n",
    "    {\"name\": \"trans_status\", \"type\": \"STRING\"},\n",
    "    {\"name\": \"department\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"quantity\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"scale\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"cost\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"unitprice\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"total\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"regprice\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"altprice\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"tax\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"taxexempt\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"foodstamp\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"wicable\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"discount\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"memdiscount\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"discountable\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"discounttype\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"voided\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"percentdiscount\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"itemqtty\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"voldisctype\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"volume\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"volspecial\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"mixmatch\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"matched\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"memtype\", \"type\": \"BOOLEAN\"},\n",
    "    {\"name\": \"staff\", \"type\": \"BOOLEAN\"},\n",
    "    {\"name\": \"numflag\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"itemstatus\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"tenderstatus\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"charflag\", \"type\": \"STRING\"},\n",
    "    {\"name\": \"varflag\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"batchheaderid\", \"type\": \"BOOLEAN\"},\n",
    "    {\"name\": \"local\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"organic\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"display\", \"type\": \"BOOLEAN\"},\n",
    "    {\"name\": \"receipt\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"card_no\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"store\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"branch\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"match_id\", \"type\": \"FLOAT\"},\n",
    "    {\"name\": \"trans_id\", \"type\": \"FLOAT\"}\n",
    "]\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "\n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Found CSV file: {file}\")\n",
    "\n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"Detected delimiter: {delimiter}\")\n",
    "\n",
    "            # Reading CSV file with correct handling of quoted fields\n",
    "            chunk_iter = pd.read_csv(full_path, delimiter=delimiter, quotechar='\"', chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "\n",
    "            table_name = file.replace('.csv', '')\n",
    "\n",
    "            # Drop the table if it exists\n",
    "            drop_table_if_exists(dataset_id, table_name, credentials, gbq_proj_id)\n",
    "\n",
    "            print(f\"Reading CSV file in chunks: {file}...\")\n",
    "            for idx, chunk_df in enumerate(chunk_iter):\n",
    "                # Clean the dataframe\n",
    "                chunk_df = clean_dataframe(chunk_df)\n",
    "\n",
    "                # Modify the field names to comply with BigQuery rules\n",
    "                chunk_df.columns = [col.lower().replace(';', '') for col in chunk_df.columns]\n",
    "\n",
    "                print(f\"Uploading chunk {idx + 1} to {table_name}...\")\n",
    "                if idx == 0:\n",
    "                    # For the first chunk, create the table with the defined schema\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='replace', credentials=credentials, table_schema=schema)\n",
    "                else:\n",
    "                    # For subsequent chunks, append to the table\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='append', credentials=credentials)\n",
    "                del chunk_df  # Clear the chunk from memory\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
