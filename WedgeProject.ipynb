{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_gbq\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths\n",
    "file_path = r\"C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\"\n",
    "\n",
    "# Read CSVs in chunks and upload to BigQuery\n",
    "credentials_path = r\"C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\wedge-project-bt-bf0ddf1029cd.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "gbq_proj_id = \"wedge-project-bt\"\n",
    "dataset_id = \"wedge_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Headers for the files\n",
    "headers = [\n",
    "    'datetime', 'register_no', 'emp_no', 'trans_no', 'upc', 'description', 'trans_type', 'trans_subtype', 'trans_status',\n",
    "    'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice', 'altPrice', 'tax', 'taxexempt', 'foodstamp',\n",
    "    'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype', 'voided', 'percentDiscount', 'ItemQtty', 'volDiscType',\n",
    "    'volume', 'VolSpecial', 'mixMatch', 'matched', 'memType', 'staff', 'numflag', 'itemstatus', 'tenderstatus', 'charflag',\n",
    "    'varflag', 'batchHeaderID', 'local', 'organic', 'display', 'receipt', 'card_no', 'store', 'branch', 'match_id', 'trans_id'\n",
    "]\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for file in files: \n",
    "        full_path = os.path.join(root, file) \n",
    "        if file.endswith('.csv'): \n",
    "            with open(full_path, 'r') as f: \n",
    "                first_line = f.readline().strip() \n",
    "\n",
    "            # Check if the file likely has headers based on the first line\n",
    "            if first_line.startswith('datetime'):\n",
    "                print(f\"File {file} seems to already have headers. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # If not, then prepend headers to the file\n",
    "            print(f\"Adding headers to {file}\")\n",
    "            with open(full_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            with open(full_path, 'w') as f:\n",
    "                f.write(','.join(headers) + '\\n' + content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunk_size = 50000  \n",
    "\n",
    "# Print directory contents for debugging\n",
    "print(f\"Contents of {file_path}:\")\n",
    "print(os.listdir(file_path))\n",
    "\n",
    "def detect_delimiter(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        first_line = file.readline()\n",
    "        if \";\" in first_line:\n",
    "            return \";\"\n",
    "        else:\n",
    "            return \",\"\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Replace \"NULL\", \"\\\\N\", \"\\\\\\\\N\", and blanks with np.NaN\n",
    "    df.replace([\"NULL\", \"\\\\N\", \"\\\\\\\\N\", \"\"], np.NaN, inplace=True)\n",
    "    \n",
    "    # Trim spaces from string columns\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "    \n",
    "    # Convert empty strings to np.NaN\n",
    "    df = df.replace(\"\", np.NaN)\n",
    "\n",
    "    # Type Conversion\n",
    "    float_columns = [\n",
    "        'register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice',\n",
    "        'altPrice', 'tax', 'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype',\n",
    "        'voided', 'percentDiscount', 'ItemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'numflag',\n",
    "        'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', 'receipt', 'card_no', 'store', 'branch', 'match_id', 'trans_id'\n",
    "    ]\n",
    "    boolean_columns = ['memType', 'staff', 'batchHeaderID', 'display']\n",
    "    string_columns = ['upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'charflag']\n",
    "\n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(float)\n",
    "\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        \n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Found CSV file: {file}\")\n",
    "            \n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"Detected delimiter: {delimiter}\")\n",
    "            \n",
    "            print(f\"Reading CSV file in chunks: {file}...\")\n",
    "            chunk_iter = pd.read_csv(full_path, delimiter=delimiter, chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "            \n",
    "            table_name = file.replace('.csv', '')  # Name the table after the CSV file\n",
    "            table_id = f\"{gbq_proj_id}.{dataset_id}.{table_name}\"\n",
    "            \n",
    "            for idx, chunk_df in enumerate(chunk_iter):\n",
    "                # Clean the dataframe\n",
    "                chunk_df = clean_dataframe(chunk_df)\n",
    "                \n",
    "                # Modify the field names to comply with BigQuery rules\n",
    "                chunk_df.columns = [col.lower().replace(';', '') for col in chunk_df.columns]\n",
    "                \n",
    "                print(f\"Uploading chunk {idx + 1} to {table_name}...\")\n",
    "                if idx == 0:\n",
    "                    # For the first chunk, create the table\n",
    "                    pandas_gbq.to_gbq(chunk_df, table_id, project_id=gbq_proj_id, if_exists='replace', credentials=credentials)\n",
    "                else:\n",
    "                    # For subsequent chunks, append to the table\n",
    "                    pandas_gbq.to_gbq(chunk_df, table_id, project_id=gbq_proj_id, if_exists='append', credentials=credentials)\n",
    "                del chunk_df  # Clear the chunk from memory\n",
    "\n",
    "print(\"Upload complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to service account JSON key file\n",
    "credentials_path = r\"C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\wedge-project-bt-bf0ddf1029cd.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "client = bigquery.Client(credentials=credentials, project= gbq_proj_id)\n",
    "\n",
    "# SQL query\n",
    "query = \"\"\"\n",
    "WITH rand_cte AS(\n",
    "SELECT DISTINCT card_no\n",
    "  FROM `wedge-project-bt.transArchive_*` \n",
    "  WHERE card_no != 3\n",
    "  ORDER BY RAND()\n",
    "  LIMIT 601)\n",
    "\n",
    "  SELECT *\n",
    "  FROM `wedge-project-bt.transArchive_*` AS trans\n",
    "  JOIN rand_cte \n",
    "  ON rand_cte.card_no = trans.card_no\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and get the result as a dataframe\n",
    "df = client.query(query).to_dataframe(create_bqstorage_client=False)\n",
    "\n",
    "# Save the dataframe to a TXT file\n",
    "df.to_csv(\"output_data1.txt\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query = \"\"\"SELECT\n",
    "  EXTRACT(DATE\n",
    "  FROM\n",
    "    datetime) AS date,\n",
    "    EXTRACT(HOUR FROM datetime) AS hour,\n",
    "  SUM(total) AS spend,\n",
    "  COUNT(DISTINCT CONCAT(EXTRACT(DATE\n",
    "        FROM\n",
    "          datetime), \n",
    "          register_no, emp_no, trans_no)) AS trans,\n",
    "  SUM(CASE\n",
    "      WHEN trans_status IN ('V', 'R') THEN -1\n",
    "    ELSE\n",
    "    1\n",
    "  END\n",
    "    ) AS items\n",
    "FROM\n",
    "  `wedge-project-bt.transArchive*`\n",
    "WHERE\n",
    "  department NOT IN (0,\n",
    "    15)\n",
    "  AND (trans_status IS  NULL\n",
    "  OR trans_status IN (' ','V','R'))\n",
    "GROUP BY\n",
    "  date, hour\n",
    "  ORDER BY \n",
    "  date, hour;\n",
    "  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to service account JSON key file\n",
    "conn = sqlite3.connect('wedge-reporting.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "GenericGBQException",
     "evalue": "Reason: 403 Access Denied: Table umt-msba:transactions.transArchive*: User does not have permission to query table umt-msba:transactions.transArchive*, or perhaps it does not exist in location US.\n\nLocation: US\nJob ID: 6eb2d261-9e58-4590-a11a-d85266a046da\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas_gbq\\gbq.py:527\u001b[0m, in \u001b[0;36mGbqConnector.run_query\u001b[1;34m(self, query, max_results, progress_bar_type, **kwargs)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     query_reply\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    528\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhttp_error \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1581\u001b[0m, in \u001b[0;36mQueryJob.result\u001b[1;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[0;32m   1579\u001b[0m         do_get_result \u001b[39m=\u001b[39m job_retry(do_get_result)\n\u001b[1;32m-> 1581\u001b[0m     do_get_result()\n\u001b[0;32m   1583\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mGoogleAPICallError \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\api_core\\retry.py:366\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m sleep_generator \u001b[39m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    364\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maximum, multiplier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiplier\n\u001b[0;32m    365\u001b[0m )\n\u001b[1;32m--> 366\u001b[0m \u001b[39mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    367\u001b[0m     target,\n\u001b[0;32m    368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predicate,\n\u001b[0;32m    369\u001b[0m     sleep_generator,\n\u001b[0;32m    370\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[0;32m    371\u001b[0m     on_error\u001b[39m=\u001b[39;49mon_error,\n\u001b[0;32m    372\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\api_core\\retry.py:204\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m target()\n\u001b[0;32m    206\u001b[0m \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1571\u001b[0m, in \u001b[0;36mQueryJob.result.<locals>.do_get_result\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1569\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_job_retry \u001b[39m=\u001b[39m job_retry\n\u001b[1;32m-> 1571\u001b[0m \u001b[39msuper\u001b[39;49m(QueryJob, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mresult(retry\u001b[39m=\u001b[39;49mretry, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1573\u001b[0m \u001b[39m# Since the job could already be \"done\" (e.g. got a finished job\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \u001b[39m# via client.get_job), the superclass call to done() might not\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \u001b[39m# set the self._query_results cache.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\bigquery\\job\\base.py:922\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[1;34m(self, retry, timeout)\u001b[0m\n\u001b[0;32m    921\u001b[0m kwargs \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m retry \u001b[39mis\u001b[39;00m DEFAULT_RETRY \u001b[39melse\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mretry\u001b[39m\u001b[39m\"\u001b[39m: retry}\n\u001b[1;32m--> 922\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(_AsyncJob, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\api_core\\future\\polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[1;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     \u001b[39m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    263\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "\u001b[1;31mForbidden\u001b[0m: 403 Access Denied: Table umt-msba:transactions.transArchive*: User does not have permission to query table umt-msba:transactions.transArchive*, or perhaps it does not exist in location US.\n\nLocation: US\nJob ID: 6eb2d261-9e58-4590-a11a-d85266a046da\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGenericGBQException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\WedgeProject.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/WedgeProject.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m holder \u001b[39m=\u001b[39m pandas_gbq\u001b[39m.\u001b[39;49mread_gbq(query, project_id\u001b[39m=\u001b[39;49mgbq_proj_id, credentials\u001b[39m=\u001b[39;49mcredentials)    \n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas_gbq\\gbq.py:943\u001b[0m, in \u001b[0;36mread_gbq\u001b[1;34m(query_or_table, project_id, index_col, col_order, reauth, auth_local_webserver, dialect, location, configuration, credentials, use_bqstorage_api, max_results, verbose, private_key, progress_bar_type, dtypes, auth_redirect_uri, client_id, client_secret)\u001b[0m\n\u001b[0;32m    928\u001b[0m connector \u001b[39m=\u001b[39m GbqConnector(\n\u001b[0;32m    929\u001b[0m     project_id,\n\u001b[0;32m    930\u001b[0m     reauth\u001b[39m=\u001b[39mreauth,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    939\u001b[0m     client_secret\u001b[39m=\u001b[39mclient_secret,\n\u001b[0;32m    940\u001b[0m )\n\u001b[0;32m    942\u001b[0m \u001b[39mif\u001b[39;00m _is_query(query_or_table):\n\u001b[1;32m--> 943\u001b[0m     final_df \u001b[39m=\u001b[39m connector\u001b[39m.\u001b[39;49mrun_query(\n\u001b[0;32m    944\u001b[0m         query_or_table,\n\u001b[0;32m    945\u001b[0m         configuration\u001b[39m=\u001b[39;49mconfiguration,\n\u001b[0;32m    946\u001b[0m         max_results\u001b[39m=\u001b[39;49mmax_results,\n\u001b[0;32m    947\u001b[0m         progress_bar_type\u001b[39m=\u001b[39;49mprogress_bar_type,\n\u001b[0;32m    948\u001b[0m         dtypes\u001b[39m=\u001b[39;49mdtypes,\n\u001b[0;32m    949\u001b[0m     )\n\u001b[0;32m    950\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    951\u001b[0m     final_df \u001b[39m=\u001b[39m connector\u001b[39m.\u001b[39mdownload_table(\n\u001b[0;32m    952\u001b[0m         query_or_table,\n\u001b[0;32m    953\u001b[0m         max_results\u001b[39m=\u001b[39mmax_results,\n\u001b[0;32m    954\u001b[0m         progress_bar_type\u001b[39m=\u001b[39mprogress_bar_type,\n\u001b[0;32m    955\u001b[0m         dtypes\u001b[39m=\u001b[39mdtypes,\n\u001b[0;32m    956\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas_gbq\\gbq.py:529\u001b[0m, in \u001b[0;36mGbqConnector.run_query\u001b[1;34m(self, query, max_results, progress_bar_type, **kwargs)\u001b[0m\n\u001b[0;32m    527\u001b[0m     query_reply\u001b[39m.\u001b[39mresult()\n\u001b[0;32m    528\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhttp_error \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m--> 529\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_http_error(ex)\n\u001b[0;32m    531\u001b[0m \u001b[39m# Avoid attempting to download results from DML queries, which have no\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[39m# destination.\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[39mif\u001b[39;00m query_reply\u001b[39m.\u001b[39mdestination \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas_gbq\\gbq.py:396\u001b[0m, in \u001b[0;36mGbqConnector.process_http_error\u001b[1;34m(ex)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[39mraise\u001b[39;00m TableCreationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReason: \u001b[39m\u001b[39m{\u001b[39;00merror_message\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    395\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 396\u001b[0m     \u001b[39mraise\u001b[39;00m GenericGBQException(\u001b[39m\"\u001b[39m\u001b[39mReason: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(ex)) \u001b[39mfrom\u001b[39;00m \u001b[39mex\u001b[39;00m\n",
      "\u001b[1;31mGenericGBQException\u001b[0m: Reason: 403 Access Denied: Table umt-msba:transactions.transArchive*: User does not have permission to query table umt-msba:transactions.transArchive*, or perhaps it does not exist in location US.\n\nLocation: US\nJob ID: 6eb2d261-9e58-4590-a11a-d85266a046da\n"
     ]
    }
   ],
   "source": [
    "holder = pandas_gbq.read_gbq(query, project_id=gbq_proj_id, credentials=credentials)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'holder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\WedgeProject.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/WedgeProject.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m holder\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'holder' is not defined"
     ]
    }
   ],
   "source": [
    "holder.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder.to.sql('date-hour.db',conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection at end of all queries\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
