{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_gbq\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import zipfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths\n",
    "file_path = r\"C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\"\n",
    "\n",
    "# Read CSVs in chunks and upload to BigQuery\n",
    "credentials_path = r\"C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project-bt-b14310631abc.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "gbq_proj_id = \"wedge-project-bt\"\n",
    "dataset_id = \"wedge_data\"\n",
    "\n",
    "\n",
    "\n",
    "# # Read CSVs in chunks and upload to BigQuery\n",
    "# credentials_path = r\"C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\umt-msba-gg-key.json\"\n",
    "# credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "# gbq_proj_id = \"umt-msba\"\n",
    "# dataset_id = \"wedge_transactions\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded:\n",
      "['transArchive_201001_201003.csv', 'transArchive_201004_201006.csv', 'transArchive_201007_201009.csv', 'transArchive_201010_201012.csv', 'transArchive_201101_201103.csv', 'transArchive_201104.csv', 'transArchive_201105.csv', 'transArchive_201106.csv', 'transArchive_201107_201109.csv', 'transArchive_201110_201112.csv', 'transArchive_201201_201203.csv', 'transArchive_201201_201203_inactive.csv', 'transArchive_201204_201206.csv', 'transArchive_201204_201206_inactive.csv', 'transArchive_201207_201209.csv', 'transArchive_201207_201209_inactive.csv', 'transArchive_201210_201212.csv', 'transArchive_201210_201212_inactive.csv', 'transArchive_201301_201303.csv', 'transArchive_201301_201303_inactive.csv', 'transArchive_201304_201306.csv', 'transArchive_201304_201306_inactive.csv', 'transArchive_201307_201309.csv', 'transArchive_201307_201309_inactive.csv', 'transArchive_201310_201312.csv', 'transArchive_201310_201312_inactive.csv', 'transArchive_201401_201403.csv', 'transArchive_201401_201403_inactive.csv', 'transArchive_201404_201406.csv', 'transArchive_201404_201406_inactive.csv', 'transArchive_201407_201409.csv', 'transArchive_201407_201409_inactive.csv', 'transArchive_201410_201412.csv', 'transArchive_201410_201412_inactive.csv', 'transArchive_201501_201503.csv', 'transArchive_201504_201506.csv', 'transArchive_201507_201509.csv', 'transArchive_201510.csv', 'transArchive_201511.csv', 'transArchive_201512.csv', 'transArchive_201601.csv', 'transArchive_201602.csv', 'transArchive_201603.csv', 'transArchive_201604.csv', 'transArchive_201605.csv', 'transArchive_201606.csv', 'transArchive_201607.csv', 'transArchive_201608.csv', 'transArchive_201609.csv', 'transArchive_201610.csv', 'transArchive_201611.csv', 'transArchive_201612.csv', 'transArchive_201701.csv']\n",
      "Found CSV file: transArchive_201001_201003.csv\n",
      "Detected delimiter: ,\n",
      "Deleted table 'wedge-project-bt.wedge_data.transArchive_201001_201003'\n",
      "Reading CSV file in chunks: transArchive_201001_201003.csv...\n",
      "Uploading chunk 1 to transArchive_201001_201003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1005.11it/s]"
     ]
    }
   ],
   "source": [
    "chunk_size = 50000  \n",
    "\n",
    "# Define the function to drop a table if it exists\n",
    "def drop_table_if_exists(dataset_id, table_name, credentials):\n",
    "    client = bigquery.Client(credentials=credentials, project=gbq_proj_id)\n",
    "    table_id = f\"{gbq_proj_id}.{dataset_id}.{table_name}\"\n",
    "    try:\n",
    "        client.delete_table(table_id)  # API request\n",
    "        print(f\"Deleted table '{table_id}'\")\n",
    "    except NotFound:\n",
    "        print(f\"Table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "# Define the detect_delimiter function here\n",
    "\n",
    "def detect_delimiter(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        first_line = file.readline()\n",
    "        if \";\" in first_line:\n",
    "            return \";\"\n",
    "        else:\n",
    "            return \",\"\n",
    "\n",
    "# Define the clean_dataframe function here\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Replace \"NULL\", \"\\\\N\", \"\\\\\\\\N\", and blanks with np.NaN\n",
    "    df.replace([\"NULL\", \"\\\\N\", \"\\\\\\\\N\", \"\"], np.NaN, inplace=True)\n",
    "    \n",
    "    # Trim spaces from string columns\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "    \n",
    "    # Convert empty strings to np.NaN\n",
    "    df = df.replace(\"\", np.NaN)\n",
    "\n",
    "    # Type Conversion\n",
    "    float_columns = [\n",
    "        'register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice',\n",
    "        'altPrice', 'tax', 'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype',\n",
    "        'voided', 'percentDiscount', 'ItemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'numflag',\n",
    "        'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', 'receipt', 'card_no', 'store', 'branch', 'match_id', 'trans_id'\n",
    "    ]\n",
    "    boolean_columns = ['memType', 'staff', 'batchHeaderID', 'display']\n",
    "    string_columns = ['upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'charflag']\n",
    "\n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(float)\n",
    "\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "print(f\"Contents of {file_path}:\")\n",
    "print(os.listdir(file_path))\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        \n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Found CSV file: {file}\")\n",
    "            \n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"Detected delimiter: {delimiter}\")\n",
    "            \n",
    "            table_name = file.replace('.csv', '')\n",
    "            \n",
    "            # Drop the table if it exists\n",
    "            drop_table_if_exists(dataset_id, table_name, credentials)  # Pass credentials here\n",
    "            \n",
    "            print(f\"Reading CSV file in chunks: {file}...\")\n",
    "            chunk_iter = pd.read_csv(full_path, delimiter=delimiter, chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "            \n",
    "            for idx, chunk_df in enumerate(chunk_iter):\n",
    "                # Clean the dataframe\n",
    "                chunk_df = clean_dataframe(chunk_df)\n",
    "                \n",
    "                # Modify the field names to comply with BigQuery rules\n",
    "                chunk_df.columns = [col.lower().replace(';', '') for col in chunk_df.columns]\n",
    "                \n",
    "                print(f\"Uploading chunk {idx + 1} to {table_name}...\")\n",
    "                if idx == 0:\n",
    "                    # For the first chunk, create the table\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='replace', credentials=credentials)\n",
    "                else:\n",
    "                    # For subsequent chunks, append to the table\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='append', credentials=credentials)\n",
    "                del chunk_df  # Clear the chunk from memory\n",
    "\n",
    "print(\"Upload complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted transArchive_201001_201003.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201004_201006.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201007_201009.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201010_201012.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201101_201103.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201104.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201105.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201106.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201107_201109.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201110_201112.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201201_201203.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201201_201203_inactive.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201204_201206.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201204_201206_inactive.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201207_201209.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201207_201209_inactive.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201210_201212.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201210_201212_inactive.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201301_201303.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201301_201303_inactive.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201304_201306.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201304_201306_inactive.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201307_201309.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201307_201309_inactive.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201310_201312.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201310_201312_inactive.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201401_201403.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201401_201403_inactive.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201404_201406.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201404_201406_inactive.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201407_201409.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201407_201409_inactive.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201410_201412.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201410_201412_inactive.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201501_201503.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201504_201506.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201507_201509.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201510.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201511.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201512.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201601.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201602.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201603.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201604.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201605.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201606.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201607.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201608.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201609.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201610.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201611.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201612.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "Extracted transArchive_201701.zip to C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded\n",
      "All files have been extracted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Define the directory where your zip files are located\n",
    "# zip_dir = \"C:\\\\Users\\\\britt\\\\OneDrive - The University of Montana\\\\Applied Data Analytics\\\\Wedge Project\\\\WedgeZipOfZips\"\n",
    "\n",
    "# # Define the directory where you want to save the unzipped files\n",
    "# extract_to_dir = \"C:\\\\Users\\\\britt\\\\OneDrive - The University of Montana\\\\Applied Data Analytics\\\\Wedge Project\\\\wedge-project\\\\Uploaded\"\n",
    "\n",
    "# # Walk through the directory\n",
    "# for root, dirs, files in os.walk(zip_dir):\n",
    "#     for file in files:\n",
    "#         if file.endswith('.zip'):\n",
    "#             # Construct the file path\n",
    "#             file_path = os.path.join(root, file)\n",
    "#             # Open the zip file\n",
    "#             with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "#                 # Extract all the contents into the directory\n",
    "#                 zip_ref.extractall(extract_to_dir)\n",
    "#                 print(f\"Extracted {file} to {extract_to_dir}\")\n",
    "\n",
    "# print(\"All files have been extracted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Headers for the files\n",
    "headers = [\n",
    "    'datetime', 'register_no', 'emp_no', 'trans_no', 'upc', 'description', 'trans_type', 'trans_subtype', 'trans_status',\n",
    "    'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice', 'altPrice', 'tax', 'taxexempt', 'foodstamp',\n",
    "    'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype', 'voided', 'percentDiscount', 'ItemQtty', 'volDiscType',\n",
    "    'volume', 'VolSpecial', 'mixMatch', 'matched', 'memType', 'staff', 'numflag', 'itemstatus', 'tenderstatus', 'charflag',\n",
    "    'varflag', 'batchHeaderID', 'local', 'organic', 'display', 'receipt', 'card_no', 'store', 'branch', 'match_id', 'trans_id'\n",
    "]\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for file in files: \n",
    "        full_path = os.path.join(root, file) \n",
    "        if file.endswith('.csv'): \n",
    "            with open(full_path, 'r') as f: \n",
    "                first_line = f.readline().strip() \n",
    "\n",
    "            # Check if the file likely has headers based on the first line\n",
    "            if first_line.startswith('datetime'):\n",
    "                print(f\"File {file} seems to already have headers. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # If not, then prepend headers to the file\n",
    "            print(f\"Adding headers to {file}\")\n",
    "            with open(full_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            with open(full_path, 'w') as f:\n",
    "                f.write(','.join(headers) + '\\n' + content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\Uploaded:\n",
      "['transArchive_201001_201003.csv', 'transArchive_201004_201006.csv', 'transArchive_201007_201009.csv', 'transArchive_201010_201012.csv', 'transArchive_201101_201103.csv', 'transArchive_201104.csv', 'transArchive_201105.csv', 'transArchive_201106.csv', 'transArchive_201107_201109.csv', 'transArchive_201110_201112.csv', 'transArchive_201201_201203.csv', 'transArchive_201201_201203_inactive.csv', 'transArchive_201204_201206.csv', 'transArchive_201204_201206_inactive.csv', 'transArchive_201207_201209.csv', 'transArchive_201207_201209_inactive.csv', 'transArchive_201210_201212.csv', 'transArchive_201210_201212_inactive.csv', 'transArchive_201301_201303.csv', 'transArchive_201301_201303_inactive.csv', 'transArchive_201304_201306.csv', 'transArchive_201304_201306_inactive.csv', 'transArchive_201307_201309.csv', 'transArchive_201307_201309_inactive.csv', 'transArchive_201310_201312.csv', 'transArchive_201310_201312_inactive.csv', 'transArchive_201401_201403.csv', 'transArchive_201401_201403_inactive.csv', 'transArchive_201404_201406.csv', 'transArchive_201404_201406_inactive.csv', 'transArchive_201407_201409.csv', 'transArchive_201407_201409_inactive.csv', 'transArchive_201410_201412.csv', 'transArchive_201410_201412_inactive.csv', 'transArchive_201501_201503.csv', 'transArchive_201504_201506.csv', 'transArchive_201507_201509.csv', 'transArchive_201510.csv', 'transArchive_201511.csv', 'transArchive_201512.csv', 'transArchive_201601.csv', 'transArchive_201602.csv', 'transArchive_201603.csv', 'transArchive_201604.csv', 'transArchive_201605.csv', 'transArchive_201606.csv', 'transArchive_201607.csv', 'transArchive_201608.csv', 'transArchive_201609.csv', 'transArchive_201610.csv', 'transArchive_201611.csv', 'transArchive_201612.csv', 'transArchive_201701.csv']\n",
      "Found CSV file: transArchive_201001_201003.csv\n",
      "Detected delimiter: ,\n"
     ]
    },
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\WedgeProject.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/WedgeProject.ipynb#X36sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m table_name \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/WedgeProject.ipynb#X36sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m# Drop the table if it exists\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/WedgeProject.ipynb#X36sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m drop_table_if_exists(dataset_id, table_name)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/WedgeProject.ipynb#X36sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReading CSV file in chunks: \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/WedgeProject.ipynb#X36sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m chunk_iter \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(full_path, delimiter\u001b[39m=\u001b[39mdelimiter, chunksize\u001b[39m=\u001b[39mchunk_size, dtype\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m, low_memory\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\WedgeProject.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/WedgeProject.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop_table_if_exists\u001b[39m(dataset_id, table_name):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/WedgeProject.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     client \u001b[39m=\u001b[39m bigquery\u001b[39m.\u001b[39;49mClient()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/WedgeProject.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     table_id \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mgbq_proj_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mdataset_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mtable_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/britt/OneDrive%20-%20The%20University%20of%20Montana/Applied%20Data%20Analytics/Wedge%20Project/wedge-project/WedgeProject.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\bigquery\\client.py:245\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, project, credentials, _http, location, default_query_job_config, default_load_job_config, client_info, client_options)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    235\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    236\u001b[0m     project\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m     client_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    244\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 245\u001b[0m     \u001b[39msuper\u001b[39;49m(Client, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    246\u001b[0m         project\u001b[39m=\u001b[39;49mproject,\n\u001b[0;32m    247\u001b[0m         credentials\u001b[39m=\u001b[39;49mcredentials,\n\u001b[0;32m    248\u001b[0m         client_options\u001b[39m=\u001b[39;49mclient_options,\n\u001b[0;32m    249\u001b[0m         _http\u001b[39m=\u001b[39;49m_http,\n\u001b[0;32m    250\u001b[0m     )\n\u001b[0;32m    252\u001b[0m     kw_args \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mclient_info\u001b[39m\u001b[39m\"\u001b[39m: client_info}\n\u001b[0;32m    253\u001b[0m     bq_host \u001b[39m=\u001b[39m _get_bigquery_host()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\client\\__init__.py:320\u001b[0m, in \u001b[0;36mClientWithProject.__init__\u001b[1;34m(self, project, credentials, client_options, _http)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, project\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, credentials\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, client_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, _http\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 320\u001b[0m     _ClientProjectMixin\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m, project\u001b[39m=\u001b[39;49mproject, credentials\u001b[39m=\u001b[39;49mcredentials)\n\u001b[0;32m    321\u001b[0m     Client\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    322\u001b[0m         \u001b[39mself\u001b[39m, credentials\u001b[39m=\u001b[39mcredentials, client_options\u001b[39m=\u001b[39mclient_options, _http\u001b[39m=\u001b[39m_http\n\u001b[0;32m    323\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\client\\__init__.py:268\u001b[0m, in \u001b[0;36m_ClientProjectMixin.__init__\u001b[1;34m(self, project, credentials)\u001b[0m\n\u001b[0;32m    265\u001b[0m     project \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(credentials, \u001b[39m\"\u001b[39m\u001b[39mproject_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    267\u001b[0m \u001b[39mif\u001b[39;00m project \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     project \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_determine_default(project)\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m project \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    272\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mProject was not passed and could not be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdetermined from the environment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\client\\__init__.py:287\u001b[0m, in \u001b[0;36m_ClientProjectMixin._determine_default\u001b[1;34m(project)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    285\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_determine_default\u001b[39m(project):\n\u001b[0;32m    286\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Helper:  use default project detection.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m _determine_default_project(project)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\cloud\\_helpers\\__init__.py:152\u001b[0m, in \u001b[0;36m_determine_default_project\u001b[1;34m(project)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Determine default project ID explicitly or implicitly as fall-back.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \n\u001b[0;32m    142\u001b[0m \u001b[39mSee :func:`google.auth.default` for details on how the default project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39m:returns: Default project if it can be determined.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m project \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 152\u001b[0m     _, project \u001b[39m=\u001b[39m google\u001b[39m.\u001b[39;49mauth\u001b[39m.\u001b[39;49mdefault()\n\u001b[0;32m    153\u001b[0m \u001b[39mreturn\u001b[39;00m project\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\auth\\_default.py:691\u001b[0m, in \u001b[0;36mdefault\u001b[1;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[0;32m    683\u001b[0m             _LOGGER\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    684\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mNo project ID could be determined. Consider running \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    685\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m`gcloud config set project` or setting the \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    686\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39menvironment variable\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    687\u001b[0m                 environment_vars\u001b[39m.\u001b[39mPROJECT,\n\u001b[0;32m    688\u001b[0m             )\n\u001b[0;32m    689\u001b[0m         \u001b[39mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[1;32m--> 691\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "chunk_size = 50000  \n",
    "\n",
    "# Define the function to drop a table if it exists\n",
    "def drop_table_if_exists(dataset_id, table_name):\n",
    "    client = bigquery.Client()\n",
    "    table_id = f\"{gbq_proj_id}.{dataset_id}.{table_name}\"\n",
    "    try:\n",
    "        client.delete_table(table_id)\n",
    "        print(f\"Deleted table '{table_id}'\")\n",
    "    except NotFound:\n",
    "        print(f\"Table '{table_id}' not found, skipping deletion.\")\n",
    "\n",
    "# Define the detect_delimiter function here\n",
    "\n",
    "def detect_delimiter(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        first_line = file.readline()\n",
    "        if \";\" in first_line:\n",
    "            return \";\"\n",
    "        else:\n",
    "            return \",\"\n",
    "\n",
    "# Define the clean_dataframe function here\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Replace \"NULL\", \"\\\\N\", \"\\\\\\\\N\", and blanks with np.NaN\n",
    "    df.replace([\"NULL\", \"\\\\N\", \"\\\\\\\\N\", \"\"], np.NaN, inplace=True)\n",
    "    \n",
    "    # Trim spaces from string columns\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "    \n",
    "    # Convert empty strings to np.NaN\n",
    "    df = df.replace(\"\", np.NaN)\n",
    "\n",
    "    # Type Conversion\n",
    "    float_columns = [\n",
    "        'register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice',\n",
    "        'altPrice', 'tax', 'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype',\n",
    "        'voided', 'percentDiscount', 'ItemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'numflag',\n",
    "        'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', 'receipt', 'card_no', 'store', 'branch', 'match_id', 'trans_id'\n",
    "    ]\n",
    "    boolean_columns = ['memType', 'staff', 'batchHeaderID', 'display']\n",
    "    string_columns = ['upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'charflag']\n",
    "\n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(float)\n",
    "\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "print(f\"Contents of {file_path}:\")\n",
    "print(os.listdir(file_path))\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        \n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Found CSV file: {file}\")\n",
    "            \n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"Detected delimiter: {delimiter}\")\n",
    "            \n",
    "            table_name = file.replace('.csv', '')\n",
    "            \n",
    "            # Drop the table if it exists\n",
    "            drop_table_if_exists(dataset_id, table_name)\n",
    "            \n",
    "            print(f\"Reading CSV file in chunks: {file}...\")\n",
    "            chunk_iter = pd.read_csv(full_path, delimiter=delimiter, chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "            \n",
    "            for idx, chunk_df in enumerate(chunk_iter):\n",
    "                # Clean the dataframe\n",
    "                chunk_df = clean_dataframe(chunk_df)\n",
    "                \n",
    "                # Modify the field names to comply with BigQuery rules\n",
    "                chunk_df.columns = [col.lower().replace(';', '') for col in chunk_df.columns]\n",
    "                \n",
    "                print(f\"Uploading chunk {idx + 1} to {table_name}...\")\n",
    "                if idx == 0:\n",
    "                    # For the first chunk, create the table\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='replace', credentials=credentials)\n",
    "                else:\n",
    "                    # For subsequent chunks, append to the table\n",
    "                    pandas_gbq.to_gbq(chunk_df, f\"{dataset_id}.{table_name}\", project_id=gbq_proj_id, if_exists='append', credentials=credentials)\n",
    "                del chunk_df  # Clear the chunk from memory\n",
    "\n",
    "print(\"Upload complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunk_size = 50000  \n",
    "\n",
    "# Print directory contents for debugging\n",
    "print(f\"Contents of {file_path}:\")\n",
    "print(os.listdir(file_path))\n",
    "\n",
    "def detect_delimiter(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        first_line = file.readline()\n",
    "        if \";\" in first_line:\n",
    "            return \";\"\n",
    "        else:\n",
    "            return \",\"\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Replace \"NULL\", \"\\\\N\", \"\\\\\\\\N\", and blanks with np.NaN\n",
    "    df.replace([\"NULL\", \"\\\\N\", \"\\\\\\\\N\", \"\"], np.NaN, inplace=True)\n",
    "    \n",
    "    # Trim spaces from string columns\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "    \n",
    "    # Convert empty strings to np.NaN\n",
    "    df = df.replace(\"\", np.NaN)\n",
    "\n",
    "    # Type Conversion\n",
    "    float_columns = [\n",
    "        'register_no', 'emp_no', 'trans_no', 'department', 'quantity', 'Scale', 'cost', 'unitPrice', 'total', 'regPrice',\n",
    "        'altPrice', 'tax', 'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', 'discountable', 'discounttype',\n",
    "        'voided', 'percentDiscount', 'ItemQtty', 'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'numflag',\n",
    "        'itemstatus', 'tenderstatus', 'varflag', 'local', 'organic', 'receipt', 'card_no', 'store', 'branch', 'match_id', 'trans_id'\n",
    "    ]\n",
    "    boolean_columns = ['memType', 'staff', 'batchHeaderID', 'display']\n",
    "    string_columns = ['upc', 'description', 'trans_type', 'trans_subtype', 'trans_status', 'charflag']\n",
    "\n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(float)\n",
    "\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "    for col in string_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for root, dirs, files in os.walk(file_path):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        \n",
    "        if file.endswith('.csv'):\n",
    "            print(f\"Found CSV file: {file}\")\n",
    "            \n",
    "            delimiter = detect_delimiter(full_path)\n",
    "            print(f\"Detected delimiter: {delimiter}\")\n",
    "            \n",
    "            print(f\"Reading CSV file in chunks: {file}...\")\n",
    "            chunk_iter = pd.read_csv(full_path, delimiter=delimiter, chunksize=chunk_size, dtype=str, low_memory=False)\n",
    "            \n",
    "            table_name = file.replace('.csv', '')  # Name the table after the CSV file\n",
    "            table_id = f\"{gbq_proj_id}.{dataset_id}.{table_name}\"\n",
    "            \n",
    "            for idx, chunk_df in enumerate(chunk_iter):\n",
    "                # Clean the dataframe\n",
    "                chunk_df = clean_dataframe(chunk_df)\n",
    "                \n",
    "                # Modify the field names to comply with BigQuery rules\n",
    "                chunk_df.columns = [col.lower().replace(';', '') for col in chunk_df.columns]\n",
    "                \n",
    "                print(f\"Uploading chunk {idx + 1} to {table_name}...\")\n",
    "                if idx == 0:\n",
    "                    # For the first chunk, create the table\n",
    "                    pandas_gbq.to_gbq(chunk_df, table_id, project_id=gbq_proj_id, if_exists='replace', credentials=credentials)\n",
    "                else:\n",
    "                    # For subsequent chunks, append to the table\n",
    "                    pandas_gbq.to_gbq(chunk_df, table_id, project_id=gbq_proj_id, if_exists='append', credentials=credentials)\n",
    "                del chunk_df  # Clear the chunk from memory\n",
    "\n",
    "print(\"Upload complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path to service account JSON key file\n",
    "# credentials_path = r\"C:\\Users\\britt\\OneDrive - The University of Montana\\Applied Data Analytics\\Wedge Project\\wedge-project\\wedge-project-bt-bf0ddf1029cd.json\"\n",
    "# credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "# client = bigquery.Client(credentials=credentials, project= gbq_proj_id)\n",
    "\n",
    "# # SQL query\n",
    "# query = \"\"\"\n",
    "# WITH rand_cte AS(\n",
    "# SELECT DISTINCT card_no\n",
    "#   FROM `wedge-project-bt.transArchive_*` \n",
    "#   WHERE card_no != 3\n",
    "#   ORDER BY RAND()\n",
    "#   LIMIT 601)\n",
    "\n",
    "#   SELECT *\n",
    "#   FROM `wedge_data.transArchive_*` AS trans\n",
    "#   JOIN rand_cte \n",
    "#   ON rand_cte.card_no = trans.card_no\n",
    "# \"\"\"\n",
    "\n",
    "# # Run the query and get the result as a dataframe\n",
    "# df = client.query(query).to_dataframe(create_bqstorage_client=False)\n",
    "\n",
    "# # Save the dataframe to a TXT file\n",
    "# df.to_csv(\"output_data1.txt\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 1\n",
    "\n",
    "query = \"\"\"SELECT\n",
    "  EXTRACT(DATE\n",
    "  FROM\n",
    "    datetime) AS date,\n",
    "    EXTRACT(HOUR FROM datetime) AS hour,\n",
    " ROUND(SUM(total),2) AS spend,\n",
    "  COUNT(DISTINCT CONCAT(EXTRACT(DATE\n",
    "        FROM\n",
    "          datetime), \n",
    "          register_no, emp_no, trans_no)) AS trans,\n",
    "  SUM(CASE\n",
    "      WHEN trans_status IN ('V', 'R') THEN -1\n",
    "    ELSE\n",
    "    1\n",
    "  END\n",
    "    ) AS items\n",
    "FROM\n",
    "  `wedge_transactions.transArchive*`\n",
    "WHERE\n",
    "  department NOT IN (0,\n",
    "    15)\n",
    "  AND (trans_status IS  NULL\n",
    "  OR trans_status IN (' ','V','R'))\n",
    "GROUP BY\n",
    "  date, hour\n",
    "  ORDER BY \n",
    "  date, hour;\n",
    "  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to service account JSON key file\n",
    "conn = sqlite3.connect('wedge-reporting.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "holder = pandas_gbq.read_gbq(query, project_id=gbq_proj_id, credentials=credentials)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>spend</th>\n",
       "      <th>trans</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>9</td>\n",
       "      <td>1006.28</td>\n",
       "      <td>36</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>3128.55</td>\n",
       "      <td>82</td>\n",
       "      <td>913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>11</td>\n",
       "      <td>4001.66</td>\n",
       "      <td>118</td>\n",
       "      <td>1108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>12</td>\n",
       "      <td>3886.51</td>\n",
       "      <td>124</td>\n",
       "      <td>1143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>13</td>\n",
       "      <td>4654.52</td>\n",
       "      <td>154</td>\n",
       "      <td>1365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  hour    spend  trans  items\n",
       "0  2010-01-01     9  1006.28     36    245\n",
       "1  2010-01-01    10  3128.55     82    913\n",
       "2  2010-01-01    11  4001.66    118   1108\n",
       "3  2010-01-01    12  3886.51    124   1143\n",
       "4  2010-01-01    13  4654.52    154   1365"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holder.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39330"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holder.to_sql('date-hour', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 2 \n",
    "\n",
    "query2 = \"\"\"SELECT DISTINCT card_no,\n",
    "   EXTRACT(YEAR FROM datetime) AS year,\n",
    "    EXTRACT(MONTH FROM datetime) AS month,\n",
    "  SUM(total) AS spend,\n",
    "  COUNT(DISTINCT CONCAT(EXTRACT(DATE\n",
    "        FROM\n",
    "          datetime), \n",
    "          register_no, emp_no, trans_no)) AS trans,\n",
    "  SUM(CASE\n",
    "      WHEN trans_status IN ('V', 'R') THEN -1\n",
    "    ELSE\n",
    "    1\n",
    "  END\n",
    "    ) AS items\n",
    "FROM\n",
    "  `wedge_transactions.transArchive*`\n",
    "WHERE\n",
    "  department NOT IN (0,\n",
    "    15)\n",
    "  AND  card_no != 3\n",
    "  AND (trans_status IS  NULL\n",
    "  OR trans_status IN (' ','V','R'))\n",
    "GROUP BY\n",
    "  card_no, year, month\n",
    "  ORDER BY \n",
    " card_no, year, month;\n",
    "  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "holder = pandas_gbq.read_gbq(query2, project_id=gbq_proj_id, credentials=credentials)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_no</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>spend</th>\n",
       "      <th>trans</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>65.87</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>53.12</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>17.34</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>60.40</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>19.65</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   card_no  year  month  spend  trans  items\n",
       "0  10000.0  2010     10  65.87      4     21\n",
       "1  10000.0  2010     11  53.12      2     20\n",
       "2  10000.0  2010     12  17.34      1      6\n",
       "3  10000.0  2011      1  60.40      4     23\n",
       "4  10000.0  2011      2  19.65      1      4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holder.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "808811"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holder.to_sql('owner-sales', conn, if_exists='replace', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 3 \n",
    "\n",
    "query3 = \"\"\"SELECT upc,\n",
    "  description,\n",
    "  w.department,\n",
    "  dept_name,\n",
    "   EXTRACT(YEAR FROM datetime) AS year,\n",
    "    EXTRACT(MONTH FROM datetime) AS month,\n",
    "  ROUND(SUM(total),2) AS sales,\n",
    "  COUNT(DISTINCT CONCAT(EXTRACT(DATE\n",
    "        FROM\n",
    "          datetime), \n",
    "          register_no, emp_no, trans_no)) AS trans,\n",
    "  SUM(CASE\n",
    "      WHEN trans_status IN ('V', 'R') THEN -1\n",
    "    ELSE\n",
    "    1\n",
    "  END\n",
    "    ) AS items\n",
    "FROM\n",
    "  `wedge_transactions.transArchive*` AS w\n",
    "JOIN `wedge_transactions.department_lookup` AS d\n",
    "ON w.department = d.department\n",
    "WHERE\n",
    "  w.department NOT IN (0,\n",
    "    15)\n",
    "  AND  card_no != 3\n",
    "  AND (trans_status IS  NULL\n",
    "  OR trans_status IN (' ','V','R'))\n",
    "GROUP BY\n",
    "  upc,description,w.department, dept_name, year, month \n",
    "ORDER BY \n",
    " sales DESC, description, year, month;\n",
    "  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "holder = pandas_gbq.read_gbq(query3, project_id=gbq_proj_id, credentials=credentials)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>upc</th>\n",
       "      <th>description</th>\n",
       "      <th>department</th>\n",
       "      <th>dept_name</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>sales</th>\n",
       "      <th>trans</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000000007025</td>\n",
       "      <td>Hot Bar Container</td>\n",
       "      <td>8.0</td>\n",
       "      <td>DELI</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>34137.02</td>\n",
       "      <td>4285</td>\n",
       "      <td>4726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000000007025</td>\n",
       "      <td>Hot Bar Container</td>\n",
       "      <td>8.0</td>\n",
       "      <td>DELI</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>33595.31</td>\n",
       "      <td>4215</td>\n",
       "      <td>4643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003338320038</td>\n",
       "      <td>O.Strawberries 16oz pkg.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>PRODUCE</td>\n",
       "      <td>2010</td>\n",
       "      <td>5</td>\n",
       "      <td>32702.24</td>\n",
       "      <td>6630</td>\n",
       "      <td>7592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000000007025</td>\n",
       "      <td>Hot Bar Container</td>\n",
       "      <td>8.0</td>\n",
       "      <td>DELI</td>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>31601.92</td>\n",
       "      <td>3982</td>\n",
       "      <td>4397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000000000338</td>\n",
       "      <td>O.Blueberries WI pkg.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>PRODUCE</td>\n",
       "      <td>2011</td>\n",
       "      <td>8</td>\n",
       "      <td>31457.30</td>\n",
       "      <td>5213</td>\n",
       "      <td>5536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             upc               description  department dept_name  year  month  \\\n",
       "0  0000000007025         Hot Bar Container         8.0      DELI  2017      1   \n",
       "1  0000000007025         Hot Bar Container         8.0      DELI  2016     12   \n",
       "2  0003338320038  O.Strawberries 16oz pkg.         2.0   PRODUCE  2010      5   \n",
       "3  0000000007025         Hot Bar Container         8.0      DELI  2016     11   \n",
       "4  0000000000338     O.Blueberries WI pkg.         2.0   PRODUCE  2011      8   \n",
       "\n",
       "      sales  trans  items  \n",
       "0  34137.02   4285   4726  \n",
       "1  33595.31   4215   4643  \n",
       "2  32702.24   6630   7592  \n",
       "3  31601.92   3982   4397  \n",
       "4  31457.30   5213   5536  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holder.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130901"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holder.to_sql('product-sales', conn, if_exists='replace', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection at end of all queries\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
